import google.generativeai as genai
import streamlit as st
import time
from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError, InvalidArgument

class AI_Core:
    def __init__(self):
        self.api_ready = False
        try:
            # Kiá»ƒm tra key tá»“n táº¡i trÆ°á»›c khi láº¥y
            if "api_keys" in st.secrets and "gemini_api_key" in st.secrets["api_keys"]:
                api_key = st.secrets["api_keys"]["gemini_api_key"]
                genai.configure(api_key=api_key)
                self.api_ready = True
            else:
                st.error("âš ï¸ ChÆ°a cáº¥u hÃ¬nh API Key trong secrets.toml")
                return

            # âœ… Cáº¥u hÃ¬nh Safety chung
            self.safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # âœ… Cáº¥u hÃ¬nh Generation Config tá»‘i Æ°u cho Gemini 2.5
            self.gen_config = genai.GenerationConfig(
                temperature=0.7,
                max_output_tokens=32768,  # 32K (2.5 há»— trá»£ Ä‘áº¿n 64K)
                top_p=0.95,
                top_k=40
            )

        except Exception as e:
            st.error(f"âŒ Lá»—i khá»Ÿi táº¡o AI Core: {e}")

    def _get_model(self, model_name, system_instr=None):
        """
        HÃ m helper Ä‘á»ƒ khá»Ÿi táº¡o model vá»›i system instruction
        
        Returns:
            GenerativeModel instance hoáº·c None náº¿u lá»—i
        """
        # âœ… TÃŠN MODEL CHÃNH XÃC CHO GEMINI 2.5
        valid_names = {
            "flash": "gemini-2.5-flash",         # á»”n Ä‘á»‹nh, nhanh
            "pro": "gemini-2.5-pro",             # Máº¡nh nháº¥t
            "exp": "gemini-2.5-flash-latest"     # Experimental (cÃ³ thá»ƒ thay Ä‘á»•i)
        }
        
        target_name = valid_names.get(model_name, "gemini-2.5-flash")
        
        try:
            return genai.GenerativeModel(
                model_name=target_name,
                safety_settings=self.safety_settings,
                generation_config=self.gen_config,
                system_instruction=system_instr
            )
        except Exception as e:
            st.warning(f"âš ï¸ KhÃ´ng thá»ƒ khá»Ÿi táº¡o model {target_name}: {e}")
            return None

    def generate(self, prompt, model_type="flash", system_instruction=None):
        """
        Generate content vá»›i fallback strategy
        
        Args:
            prompt: User prompt
            model_type: "flash", "pro", hoáº·c "exp"
            system_instruction: System instruction (optional)
        
        Returns:
            str: Generated text hoáº·c error message
        """
        if not self.api_ready:
            return "âš ï¸ API Key chÆ°a sáºµn sÃ ng."

        # âœ… Chiáº¿n lÆ°á»£c fallback thÃ´ng minh
        if model_type == "pro":
            # Task phá»©c táº¡p: Pro â†’ Flash â†’ Exp
            plan = [("pro", "Pro", 4), ("flash", "Flash", 2), ("exp", "Exp", 5)]
        else:
            # Task thÆ°á»ng: Flash â†’ Exp â†’ Pro (tiáº¿t kiá»‡m chi phÃ­)
            plan = [("flash", "Flash", 2), ("exp", "Exp", 3), ("pro", "Pro", 5)]

        last_errors = []
        quota_exhausted_count = 0  # âœ… Äáº¿m sá»‘ láº§n háº¿t quota

        for m_type, m_name, base_wait_time in plan:
            try:
                # Khá»Ÿi táº¡o model
                model = self._get_model(m_type, system_instr=system_instruction)
                if not model:
                    continue  # Skip náº¿u khÃ´ng khá»Ÿi táº¡o Ä‘Æ°á»£c
                
                # âœ… Gá»i API
                response = model.generate_content(prompt)
                
                # âœ… KIá»‚M TRA RESPONSE Äáº¦Y Äá»¦
                if response and hasattr(response, 'text') and response.text:
                    return response.text
                
                # âœ… Xá»¬ LÃ CÃC TRÆ¯á»œNG Há»¢P Äáº¶C BIá»†T
                if response and hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    
                    if hasattr(candidate, 'finish_reason'):
                        reason = candidate.finish_reason.name
                        
                        if reason == "SAFETY":
                            error_msg = f"{m_name}: Response bá»‹ cháº·n bá»Ÿi Safety Filter"
                            last_errors.append(error_msg)
                            continue  # Thá»­ model khÃ¡c
                            
                        elif reason == "MAX_TOKENS":
                            error_msg = f"{m_name}: Response quÃ¡ dÃ i (vÆ°á»£t max_tokens)"
                            last_errors.append(error_msg)
                            # Thá»­ model khÃ¡c vá»›i context ngáº¯n hÆ¡n
                            continue
                
                # Náº¿u khÃ´ng cÃ³ text nhÆ°ng khÃ´ng cÃ³ lá»—i cá»¥ thá»ƒ
                error_msg = f"{m_name}: Response rá»—ng (unknown reason)"
                last_errors.append(error_msg)
                continue
            
            except ResourceExhausted:
                quota_exhausted_count += 1
                error_msg = f"{m_name}: Háº¿t quota (429)"
                last_errors.append(error_msg)
                
                # âœ… EXPONENTIAL BACKOFF
                backoff = base_wait_time * (2 ** (quota_exhausted_count - 1))
                backoff = min(backoff, 30)  # Tá»‘i Ä‘a 30s
                
                time.sleep(backoff)
                
            except (ServiceUnavailable, InternalServerError) as e:
                error_msg = f"{m_name}: Lá»—i server Google (5xx)"
                last_errors.append(error_msg)
                time.sleep(1)  # Retry nhanh cho lá»—i táº¡m thá»i
            
            except InvalidArgument as e:
                # âœ… Lá»—i input khÃ´ng nÃªn retry
                return f"âš ï¸ Lá»—i Input (prompt khÃ´ng há»£p lá»‡): {str(e)[:200]}"
                
            except Exception as e:
                error_msg = f"{m_name}: {str(e)[:100]}"
                last_errors.append(error_msg)
                time.sleep(1)

        # âœ… Táº¤T Cáº¢ MODEL Äá»€U FAIL
        error_summary = "\n".join(f"- {e}" for e in last_errors[-3:])  # Chá»‰ hiá»‡n 3 lá»—i cuá»‘i
        return f"âš ï¸ Há»‡ thá»‘ng quÃ¡ táº£i hoáº·c lá»—i nghiÃªm trá»ng:\n{error_summary}\n\nğŸ’¡ Vui lÃ²ng thá»­ láº¡i sau 1-2 phÃºt."

    @staticmethod
    @st.cache_data(show_spinner=False, ttl=3600)
    def analyze_static(text, instruction):
        """
        âœ… HÃ m phÃ¢n tÃ­ch tÄ©nh vá»›i cache (cho RAG)
        
        Static method Ä‘á»ƒ Streamlit cache Ä‘Ãºng cÃ¡ch
        """
        try:
            # Láº¥y API key má»—i láº§n gá»i (vÃ¬ static khÃ´ng cÃ³ self)
            api_key = st.secrets["api_keys"]["gemini_api_key"]
            genai.configure(api_key=api_key)
            
            # âœ… DÃ¹ng Flash cho RAG (nhanh + ráº»)
            model = genai.GenerativeModel(
                "gemini-2.5-flash",
                system_instruction=instruction,
                safety_settings=[
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
            )
            
            # âœ… Giá»›i háº¡n input Ä‘á»ƒ trÃ¡nh lá»—i token (2.5 cÃ³ context 2M tokens nhÆ°ng váº«n nÃªn giá»›i háº¡n)
            max_chars = 200000  # ~50K tokens
            truncated_text = text[:max_chars]
            
            if len(text) > max_chars:
                st.warning(f"âš ï¸ Text quÃ¡ dÃ i. Chá»‰ phÃ¢n tÃ­ch {max_chars:,} kÃ½ tá»± Ä‘áº§u.")
            
            response = model.generate_content(truncated_text)
            
            if response and hasattr(response, 'text') and response.text:
                return response.text
            else:
                return "âš ï¸ KhÃ´ng cÃ³ response tá»« AI"
                
        except Exception as e:
            return f"âŒ Lá»—i phÃ¢n tÃ­ch tÄ©nh: {str(e)[:200]}"
